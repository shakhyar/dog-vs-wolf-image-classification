# -*- coding: utf-8 -*-
"""dogs_vs_wolves_classification (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yeFdgSUB8qByFUuHfOOk9YZfFpRoqVDR
"""

#!wget http://vision.stanford.edu/aditya86/ImageNetDogs/images.tar

import matplotlib.pyplot as plt
import numpy as np
import os
import PIL
import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential

from google.colab import drive
drive.mount('/content/gdrive')

os.environ['KAGGLE_CONFIG_DIR'] = "/content/gdrive/My Drive/Kaggle"
# /content/gdrive/My Drive/Kaggle is the path where kaggle.json is present in the Google Drive

# Commented out IPython magic to ensure Python compatibility.
#changing the working directory
# %cd /content/gdrive/My Drive/Kaggle
#Check the present working directory using pwd command

!kaggle datasets download -d harishvutukuri/dogs-vs-wolves

#unzipping the zip files and deleting the zip files
#!unzip \*.zip  && rm *.zip

import pathlib
data_dir = tf.keras.utils.get_file('/content/gdrive/MyDrive/Kaggle/data', origin="/content/gdrive/MyDrive/Kaggle/data")
data_dir = pathlib.Path(data_dir)

dogs = list(data_dir.glob('dogs/*'))
PIL.Image.open(str(dogs[100]))

batch_size = 32
img_height = 300
img_width = 300
epochs = 40
num_classes = 2

train_ds = tf.keras.preprocessing.image_dataset_from_directory(
    data_dir,
    validation_split=0.2,
    subset="training",
    seed=128,
    image_size=(img_height, img_width)
)
val_ds = tf.keras.preprocessing.image_dataset_from_directory(
    data_dir,
    validation_split=0.2,
    subset="validation",
    seed=128,
    image_size=(img_height,img_width)
)
class_names = train_ds.class_names
print(class_names)

AUTOTUNE = tf.data.AUTOTUNE

train_ds = train_ds.cache().shuffle(100).prefetch(buffer_size=AUTOTUNE)
val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)

augmentation_layer = keras.Sequential(
    [
     layers.experimental.preprocessing.RandomFlip("horizontal",
                                    input_shape=(img_height, img_width, 3)),
     layers.experimental.preprocessing.RandomRotation(0.1),
     layers.experimental.preprocessing.RandomRotation(0.2),
     layers.experimental.preprocessing.RandomZoom(0.2)
    ]
)

normalization_layer = layers.experimental.preprocessing.Rescaling(1./255)

model = keras.Sequential(
    [
     augmentation_layer,
     normalization_layer,
     layers.Conv2D(16, 3, padding='same', activation='relu'),
     layers.MaxPooling2D(),
     layers.Conv2D(32, 3, padding='same', activation='relu'),
     layers.MaxPooling2D(),
     layers.Conv2D(64, 3, padding='same', activation='relu'),
     layers.MaxPooling2D(),
     layers.Dropout(0.1),
     layers.Flatten(),
     layers.Dense(128, activation='relu'),
     layers.Dense(num_classes)
    ]
)

model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])
model.summary()

history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=epochs
)

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(epochs)

plt.figure(figsize=(8, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()

test_url = "https://dogzone-tcwebsites.netdna-ssl.com/wp-content/uploads/2018/07/siberian-husky-price.jpg"
test_path = tf.keras.utils.get_file('siberian-husky-price.jpg', origin=test_url)

img = keras.preprocessing.image.load_img(
    test_path, target_size=(img_height, img_width)
)
img_array = keras.preprocessing.image.img_to_array(img)
img_array = tf.expand_dims(img_array, 0) # Create a batch

predictions = model.predict(img_array)
score = tf.nn.softmax(predictions[0])

print(
    "Predocted: {} ... Accuracy: {:.2f}% ... Actual: Siberian Husky"
    .format(class_names[np.argmax(score)], 100 * np.max(score))
)

test_url = "https://ichef.bbci.co.uk/news/1024/cpsprodpb/2124/production/_106348480_mediaitem106348478.jpg"
test_path = tf.keras.utils.get_file('_106348480_mediaitem106348478.jpg', origin=test_url)

img = keras.preprocessing.image.load_img(
    test_path, target_size=(img_height, img_width)
)
img_array = keras.preprocessing.image.img_to_array(img)
img_array = tf.expand_dims(img_array, 0) # Create a batch

predictions = model.predict(img_array)
score = tf.nn.softmax(predictions[0])

print(
    "Predocted: {} ... Accuracy: {:.2f}% ... Actual: Rare Wolf"
    .format(class_names[np.argmax(score)], 100 * np.max(score))
)